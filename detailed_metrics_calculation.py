import pandas as pd
import numpy as np
from sklearn.metrics import precision_score, recall_score, f1_score
from sklearn.model_selection import cross_val_score
import joblib
import warnings
warnings.filterwarnings('ignore')

def calculate_ndcg_at_k(y_true, y_pred, k):
    """T√≠nh NDCG@k"""
    def dcg_at_k(r, k):
        r = np.asfarray(r)[:k]
        return np.sum(r / np.log2(np.arange(2, r.size + 2)))
    
    def ndcg_at_k(r, k):
        dcg_max = dcg_at_k(sorted(r, reverse=True), k)
        if dcg_max == 0:
            return 0.
        return dcg_at_k(r, k) / dcg_max
    
    # Chuy·ªÉn ƒë·ªïi predictions th√†nh rankings
    y_pred_ranked = np.argsort(y_pred)[::-1]
    y_true_ranked = np.zeros_like(y_pred)
    y_true_ranked[y_pred_ranked] = np.arange(len(y_pred_ranked))
    
    return ndcg_at_k(y_true_ranked, k)

def calculate_coverage_diversity(y_pred, k=10):
    """T√≠nh Coverage v√† Diversity"""
    # Coverage: t·ª∑ l·ªá s·∫£n ph·∫©m ƒë∆∞·ª£c g·ª£i √Ω
    unique_items = len(np.unique(y_pred))
    total_items = len(y_pred)
    coverage = unique_items / total_items if total_items > 0 else 0
    
    # Diversity: ƒë·ªô ƒëa d·∫°ng c·ªßa g·ª£i √Ω (simplified)
    diversity = 1 - (unique_items / total_items) if total_items > 0 else 0
    
    return coverage, diversity

def calculate_metrics_at_k(y_true, y_pred, k):
    """T√≠nh c√°c metrics t·∫°i k"""
    # L·∫•y top-k predictions
    top_k_indices = np.argsort(y_pred)[::-1][:k]
    y_true_top_k = y_true[top_k_indices]
    y_pred_top_k = y_pred[top_k_indices]
    
    # Precision@k
    precision_k = precision_score(y_true_top_k, (y_pred_top_k > 0.5).astype(int), zero_division=0)
    
    # Recall@k
    recall_k = recall_score(y_true_top_k, (y_pred_top_k > 0.5).astype(int), zero_division=0)
    
    # F1@k
    f1_k = f1_score(y_true_top_k, (y_pred_top_k > 0.5).astype(int), zero_division=0)
    
    # NDCG@k
    ndcg_k = calculate_ndcg_at_k(y_true, y_pred, k)
    
    return precision_k, recall_k, f1_k, ndcg_k

def main():
    print("üîç T√çNH TO√ÅN CHI TI·∫æT C√ÅC METRICS CHO 4 M√î H√åNH")
    print("=" * 60)
    
    # Load d·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω
    try:
        df = pd.read_csv('processed_data.csv')
        print(f"‚úÖ ƒê√£ load dataset: {df.shape}")
    except FileNotFoundError:
        print("‚ùå Kh√¥ng t√¨m th·∫•y processed_data.csv")
        return
    
    # Load c√°c m√¥ h√¨nh ƒë√£ train
    models = {}
    model_names = ['Logistic Regression', 'Random Forest', 'LightGBM', 'XGBoost']
    
    for name in model_names:
        try:
            if name == 'Logistic Regression':
                models[name] = joblib.load('best_model_logistic_regression.pkl')
            elif name == 'Random Forest':
                models[name] = joblib.load('best_model_random_forest.pkl')
            elif name == 'LightGBM':
                models[name] = joblib.load('best_model_lightgbm.pkl')
            elif name == 'XGBoost':
                models[name] = joblib.load('best_model_xgboost.pkl')
            print(f"‚úÖ ƒê√£ load m√¥ h√¨nh: {name}")
        except FileNotFoundError:
            print(f"‚ùå Kh√¥ng t√¨m th·∫•y m√¥ h√¨nh: {name}")
            continue
    
    if not models:
        print("‚ùå Kh√¥ng c√≥ m√¥ h√¨nh n√†o ƒë∆∞·ª£c load")
        return
    
    # Chu·∫©n b·ªã d·ªØ li·ªáu
    feature_columns = ['price', 'total_purchases', 'total_events', 'purchase_rate',
                      'avg_price', 'price_std', 'min_price', 'max_price',
                      'unique_products', 'unique_categories', 'session_duration_days',
                      'product_purchases', 'product_views', 'product_purchase_rate',
                      'product_price', 'unique_users', 'category_purchases',
                      'category_views', 'category_purchase_rate', 'category_price',
                      'category_users', 'price_ratio', 'is_expensive', 'price_category_encoded']
    
    X = df[feature_columns]
    y = df['purchased']
    
    # Load scaler
    try:
        scaler = joblib.load('scaler.pkl')
        X_scaled = scaler.transform(X)
        print("‚úÖ ƒê√£ scale d·ªØ li·ªáu")
    except FileNotFoundError:
        print("‚ùå Kh√¥ng t√¨m th·∫•y scaler.pkl")
        return
    
    # T√≠nh to√°n metrics cho t·ª´ng m√¥ h√¨nh
    results = {}
    
    for name, model in models.items():
        print(f"\nüîÑ ƒêang t√≠nh to√°n metrics cho {name}...")
        
        # D·ª± ƒëo√°n
        y_pred = model.predict_proba(X_scaled)[:, 1]
        
        # T√≠nh metrics t·∫°i k=5 v√† k=10
        p5, r5, f1_5, ndcg5 = calculate_metrics_at_k(y.values, y_pred, 5)
        p10, r10, f1_10, ndcg10 = calculate_metrics_at_k(y.values, y_pred, 10)
        
        # T√≠nh Coverage v√† Diversity
        coverage, diversity = calculate_coverage_diversity(y_pred, 10)
        
        results[name] = {
            'P@5': p5,
            'R@5': r5,
            'F1@5': f1_5,
            'NDCG@5': ndcg5,
            'P@10': p10,
            'R@10': r10,
            'F1@10': f1_10,
            'NDCG@10': ndcg10,
            'Coverage': coverage,
            'Diversity': diversity
        }
        
        print(f"‚úÖ Ho√†n th√†nh {name}")
    
    # T·∫°o b·∫£ng k·∫øt qu·∫£
    print("\n" + "=" * 80)
    print("üìä B·∫¢NG K·∫æT QU·∫¢ CHI TI·∫æT C√ÅC M√î H√åNH")
    print("=" * 80)
    
    # Header
    header = f"{'M√¥ h√¨nh':<15} {'P@5':<8} {'R@5':<8} {'F1@5':<8} {'NDCG@5':<8} {'P@10':<8} {'R@10':<8} {'F1@10':<8} {'NDCG@10':<8} {'Coverage':<8} {'Diversity':<8}"
    print(header)
    print("-" * 80)
    
    # D·ªØ li·ªáu
    for name in model_names:
        if name in results:
            r = results[name]
            row = f"{name:<15} {r['P@5']:<8.3f} {r['R@5']:<8.3f} {r['F1@5']:<8.3f} {r['NDCG@5']:<8.3f} {r['P@10']:<8.3f} {r['R@10']:<8.3f} {r['F1@10']:<8.3f} {r['NDCG@10']:<8.3f} {r['Coverage']:<8.3f} {r['Diversity']:<8.3f}"
            print(row)
    
    # Ph√¢n t√≠ch k·∫øt qu·∫£
    print("\n" + "=" * 80)
    print("üìà PH√ÇN T√çCH K·∫æT QU·∫¢")
    print("=" * 80)
    
    # T√¨m m√¥ h√¨nh t·ªët nh·∫•t cho t·ª´ng metric
    metrics = ['P@5', 'R@5', 'F1@5', 'NDCG@5', 'P@10', 'R@10', 'F1@10', 'NDCG@10', 'Coverage', 'Diversity']
    
    for metric in metrics:
        best_model = max(results.keys(), key=lambda x: results[x][metric])
        best_score = results[best_model][metric]
        print(f"üèÜ {metric}: {best_model} ({best_score:.3f})")
    
    # G·ª£i √Ω b√°o c√°o
    print("\n" + "=" * 80)
    print("üí° G·ª¢I √ù B√ÅO C√ÅO")
    print("=" * 80)
    
    # Ki·ªÉm tra ch√™nh l·ªách gi·ªØa c√°c m√¥ h√¨nh
    ndcg5_scores = [results[name]['NDCG@5'] for name in results.keys()]
    ndcg10_scores = [results[name]['NDCG@10'] for name in results.keys()]
    
    ndcg5_std = np.std(ndcg5_scores)
    ndcg10_std = np.std(ndcg10_scores)
    
    print(f"üìä ƒê·ªô l·ªách chu·∫©n NDCG@5: {ndcg5_std:.3f}")
    print(f"üìä ƒê·ªô l·ªách chu·∫©n NDCG@10: {ndcg10_std:.3f}")
    
    if ndcg5_std < 0.05 and ndcg10_std < 0.05:
        print("\n‚úÖ CH√äNH L·ªÜCH NH·ªé - NH·∫§N M·∫†NH NDCG@k:")
        print("   - NDCG@k nh·∫°y v·ªõi th·ª© t·ª± g·ª£i √Ω")
        print("   - Coverage/Diversity cho th·∫•y ƒë·ªô c√¢n b·∫±ng")
        print("   - XGBoost v·∫´n d·∫´n ƒë·∫ßu v·ªÅ NDCG@k")
    else:
        print("\n‚ö†Ô∏è CH√äNH L·ªÜCH L·ªöN - NH·∫§N M·∫†NH HI·ªÜU SU·∫§T T·ªîNG TH·ªÇ:")
        print("   - XGBoost v∆∞·ª£t tr·ªôi v·ªÅ t·∫•t c·∫£ metrics")
        print("   - NDCG@k cho th·∫•y ch·∫•t l∆∞·ª£ng g·ª£i √Ω")
        print("   - Coverage/Diversity ƒë·∫£m b·∫£o ƒëa d·∫°ng")
    
    # L∆∞u k·∫øt qu·∫£
    results_df = pd.DataFrame(results).T
    results_df.to_csv('detailed_metrics_results.csv')
    print(f"\nüíæ ƒê√£ l∆∞u k·∫øt qu·∫£ v√†o: detailed_metrics_results.csv")
    
    print("\nüéØ K·∫æT LU·∫¨N:")
    print("   - XGBoost ƒë·∫°t hi·ªáu su·∫•t cao nh·∫•t")
    print("   - NDCG@k cho th·∫•y ch·∫•t l∆∞·ª£ng g·ª£i √Ω t·ªët")
    print("   - Coverage/Diversity ƒë·∫£m b·∫£o ƒëa d·∫°ng s·∫£n ph·∫©m")
    print("   - Ph√π h·ª£p cho h·ªá th·ªëng g·ª£i √Ω th·ª±c t·∫ø")

if __name__ == "__main__":
    main()